{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","private_outputs":true,"authorship_tag":"ABX9TyO7zf2HZyE6xd8m7QdDi/aq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!!pip install -q git+https://github.com/keras-team/keras-hub.git\n","!!pip install -q --upgrade keras  # Upgrade to Keras 3."],"metadata":{"id":"SPSv48wIo425"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n","\n","import json\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import keras\n","from keras import losses\n","from keras import ops\n","from keras import optimizers\n","from keras.optimizers import schedules\n","from keras import metrics\n","from keras.applications.imagenet_utils import decode_predictions\n","import keras_hub\n","\n","# Import tensorflow for `tf.data` and its preprocessing functions\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n"],"metadata":{"id":"ipNRoNRLo8II"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = keras_hub.models.ImageClassifier.from_preset(\"resnet_v2_50_imagenet\")"],"metadata":{"id":"wOID3_6_o-eG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lr_warmup_cosine_decay(\n","    global_step,\n","    warmup_steps,\n","    hold=0,\n","    total_steps=0,\n","    start_lr=0.0,\n","    target_lr=1e-2,\n","):\n","    # Cosine decay\n","    learning_rate = (\n","        0.5\n","        * target_lr\n","        * (\n","            1\n","            + ops.cos(\n","                math.pi\n","                * ops.convert_to_tensor(\n","                    global_step - warmup_steps - hold, dtype=\"float32\"\n","                )\n","                / ops.convert_to_tensor(\n","                    total_steps - warmup_steps - hold, dtype=\"float32\"\n","                )\n","            )\n","        )\n","    )\n","\n","    warmup_lr = target_lr * (global_step / warmup_steps)\n","\n","    if hold > 0:\n","        learning_rate = ops.where(\n","            global_step > warmup_steps + hold, learning_rate, target_lr\n","        )\n","\n","    learning_rate = ops.where(global_step < warmup_steps, warmup_lr, learning_rate)\n","    return learning_rate\n","\n","\n","class WarmUpCosineDecay(schedules.LearningRateSchedule):\n","    def __init__(self, warmup_steps, total_steps, hold, start_lr=0.0, target_lr=1e-2):\n","        super().__init__()\n","        self.start_lr = start_lr\n","        self.target_lr = target_lr\n","        self.warmup_steps = warmup_steps\n","        self.total_steps = total_steps\n","        self.hold = hold\n","\n","    def __call__(self, step):\n","        lr = lr_warmup_cosine_decay(\n","            global_step=step,\n","            total_steps=self.total_steps,\n","            warmup_steps=self.warmup_steps,\n","            start_lr=self.start_lr,\n","            target_lr=self.target_lr,\n","            hold=self.hold,\n","        )\n","        return ops.where(step > self.total_steps, 0.0, lr)"],"metadata":{"id":"yEPUzMHOprxS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"67GhTvAOxPSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Parameters\n","BATCH_SIZE = 32\n","IMAGE_SIZE = (256, 256)\n","AUTOTUNE = tf.data.AUTOTUNE\n","NUM_CLASSES = 4\n","NUM_FOLDS = 5\n","EPOCHS = 30\n","GAMMA = 1.5\n"],"metadata":{"id":"_rZaBEwVvvTl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","    \"/content/drive/MyDrive/praeciso/tt\",\n","    image_size=IMAGE_SIZE,\n","    batch_size=1,\n","    label_mode='int'\n",")\n","\n","images = []\n","labels = []\n","for image, label in dataset:\n","    images.append(image[0])\n","    labels.append(label[0])\n","\n","\n","images = np.array([img.numpy() for img in images])\n","labels = np.array([lbl.numpy() for lbl in labels])\n","\n","# Check the shape to confirm\n","print(\"Images shape:\", images.shape)\n","print(\"Labels shape:\", labels.shape)"],"metadata":{"id":"m_49icrHD5Ya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import matplotlib.pyplot as plt\n","\n","def augment_dataset(images, labels, gamma_value):\n","    gamma_images = tf.image.adjust_gamma(images, gamma=gamma_value).numpy()\n","    augmented_images = np.concatenate((images, gamma_images), axis=0)\n","    augmented_labels = np.concatenate((labels, labels), axis=0)\n","    return augmented_images, augmented_labels\n","\n","# Define function to visualize gamma augmentation\n","def visualize_gamma_augmentation(images, gamma_value):\n","    gamma_images = tf.image.adjust_gamma(images, gamma=gamma_value).numpy()\n","\n","    # Display original and gamma-corrected images side-by-side\n","    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","    for i in range(5):\n","        axes[0, i].imshow(images[i].astype(\"uint8\"))\n","        axes[0, i].axis(\"off\")\n","        axes[0, i].set_title(\"Original\")\n","\n","        axes[1, i].imshow(gamma_images[i].astype(\"uint8\"))\n","        axes[1, i].axis(\"off\")\n","        axes[1, i].set_title(f\"Gamma: {gamma_value}\")\n","\n","    plt.suptitle(\"Gamma Augmentation Visualization\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Apply gamma augmentation and print dataset sizes\n","def augment_and_check(images, labels, gamma_value):\n","    print(f\"Original dataset size: {len(images)} images\")\n","    augmented_images, augmented_labels = augment_dataset(images, labels, gamma_value)\n","    print(f\"Dataset size after augmentation: {len(augmented_images)} images\")\n","\n","    # Visualize augmentation\n","    visualize_gamma_augmentation(images, gamma_value)\n","    return augmented_images, augmented_labels\n","\n","# Example: Validate gamma augmentation with gamma=1.5\n","gamma_value_to_check = 1.5\n","augmented_images, augmented_labels = augment_and_check(images[:10], labels[:10], gamma_value_to_check)  # Use a subset for visualization\n"],"metadata":{"id":"v6yVTO4ZNN90"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","\n","# Hyperparameters and initialization\n","gamma_values = [1.2, 1.4, 1.5]  # Gamma values to test\n","batch_size = 64\n","BATCH_SIZE = 64\n","EPOCHS = 15\n","NUM_CLASSES = 4\n","AUTOTUNE = tf.data.AUTOTUNE\n","IMAGE_SIZE = (224, 224)  # Define image size for resizing\n","\n","# Nested CV setup\n","outer_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","# Store results\n","outer_results = []\n","validation_histories = []  # Track validation accuracy for each gamma\n","\n","# Define function for dataset augmentation\n","def augment_dataset(images, labels, gamma_value):\n","    gamma_images = tf.image.adjust_gamma(images, gamma=gamma_value).numpy()\n","    augmented_images = np.concatenate((images, gamma_images), axis=0)\n","    augmented_labels = np.concatenate((labels, labels), axis=0)\n","    return augmented_images, augmented_labels\n","\n","# Load and preprocess dataset\n","def load_dataset(directory, image_size):\n","    dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        directory,\n","        image_size=image_size,\n","        label_mode=\"int\",\n","        batch_size=None,  # No batching here, we'll handle batching later\n","    )\n","    images = []\n","    labels = []\n","    for image, label in dataset:\n","        images.append(image)\n","        labels.append(label)\n","    images = np.stack(images)\n","    labels = np.array(labels)\n","    return images, labels\n","\n","# Read dataset\n","images, labels = load_dataset(\"/content/drive/MyDrive/praeciso/tt\", IMAGE_SIZE)\n","print(\"Images shape:\", images.shape)\n","print(\"Labels shape:\", labels.shape)\n","\n","# Outer CV loop\n","for outer_fold, (train_val_idx, test_idx) in enumerate(outer_cv.split(images)):\n","    gamma_value = gamma_values[outer_fold % len(gamma_values)]  # Assign gamma value for this fold\n","    print(f\"Outer Fold {outer_fold + 1}: Using Gamma Value = {gamma_value}\")\n","\n","    # Split data into train+val and test sets\n","    X_train_val, X_test = images[train_val_idx], images[test_idx]\n","    y_train_val, y_test = labels[train_val_idx], labels[test_idx]\n","\n","    # Augment the training+validation dataset with the selected gamma value\n","    aug_train_val_images, aug_train_val_labels = augment_dataset(X_train_val, y_train_val, gamma_value)\n","\n","    best_inner_model = None\n","    best_inner_accuracy = 0\n","    fold_validation_accuracies = []\n","\n","    # Inner CV loop\n","    for inner_fold, (train_idx, val_idx) in enumerate(inner_cv.split(aug_train_val_images)):\n","        # Split into train and validation sets\n","        X_train, X_val = aug_train_val_images[train_idx], aug_train_val_images[val_idx]\n","        y_train, y_val = aug_train_val_labels[train_idx], aug_train_val_labels[val_idx]\n","\n","        # Create TF datasets\n","        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","        train_dataset = train_dataset.batch(batch_size).prefetch(AUTOTUNE)\n","\n","        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n","        val_dataset = val_dataset.batch(batch_size).prefetch(AUTOTUNE)\n","\n","        # Define and compile the model\n","        base_model = keras_hub.models.ImageClassifier.from_preset(\"resnet_v2_50_imagenet\", num_classes=4, trainable=True)\n","        inputs = tf.keras.Input(shape=(224, 224, 3))\n","        x = base_model(inputs)\n","        outputs = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n","        model = tf.keras.Model(inputs, outputs)\n","\n","        print(\"Length of augmented training + validation images:\", len(aug_train_val_images))\n","        print(\"Length of augmented training + validation labels:\", len(aug_train_val_labels))\n","\n","        total_images = len(aug_train_val_images)\n","        total_steps = (total_images // BATCH_SIZE) * EPOCHS\n","        warmup_steps = int(0.1 * total_steps)\n","        hold_steps = int(0.45 * total_steps)\n","        schedule = WarmUpCosineDecay(\n","            start_lr=0.05,\n","            target_lr=1e-2,\n","            warmup_steps=warmup_steps,\n","            total_steps=total_steps,\n","            hold=hold_steps,\n","        )\n","        model.compile(\n","            loss=\"sparse_categorical_crossentropy\",\n","            optimizer=keras.optimizers.SGD(weight_decay=5e-4, learning_rate=schedule, momentum=0.9,),\n","            metrics=[\"accuracy\"],\n","        )\n","\n","        # Train the model\n","        history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, verbose=1)\n","\n","        # Store validation accuracy for plotting\n","        fold_validation_accuracies.append(history.history[\"val_accuracy\"])\n","\n","        # Evaluate validation accuracy\n","        val_accuracy = history.history[\"val_accuracy\"][-1]\n","        print(f\"Inner Fold {inner_fold + 1}: Validation Accuracy = {val_accuracy}\")\n","\n","        # Update best model for inner loop\n","        if val_accuracy > best_inner_accuracy:\n","            best_inner_accuracy = val_accuracy\n","            best_inner_model = model.get_weights()\n","\n","    # Track validation history for visualization\n","    validation_histories.append(fold_validation_accuracies)\n","\n","    # Test the best model from inner CV on the outer test set\n","    best_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","    best_model.set_weights(best_inner_model)\n","\n","    # Recompile the model after loading weights\n","    best_model.compile(\n","        loss=\"sparse_categorical_crossentropy\",\n","        optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n","        metrics=[\"accuracy\"],\n","    )\n","\n","    # Evaluate on the test set\n","    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n","    test_dataset = test_dataset.batch(batch_size).prefetch(AUTOTUNE)\n","    test_accuracy = best_model.evaluate(test_dataset, verbose=0)[1]\n","    print(f\"Outer Fold {outer_fold + 1}: Test Accuracy = {test_accuracy}\")\n","    outer_results.append(test_accuracy)\n","\n","    # Plot validation accuracy progression\n","    plt.figure(figsize=(10, 5))\n","    for idx, gamma_acc in enumerate(fold_validation_accuracies):\n","        plt.plot(gamma_acc, label=f\"Inner Fold {idx + 1}\")\n","    plt.title(f\"Outer Fold {outer_fold + 1}: Validation Accuracy Progression\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend()\n","    plt.show()\n","\n","# Final results\n","print(f\"Nested CV Average Accuracy: {np.mean(outer_results)}\")\n"],"metadata":{"id":"kivREARWD06C"},"execution_count":null,"outputs":[]}]}